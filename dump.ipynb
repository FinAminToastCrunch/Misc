{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# ! pip install tensorly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import tucker\n",
    "from tqdm import tqdm\n",
    "# from tensorly.metrics import norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IdentityModelPlusNoise(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch model that returns its input unchanged.\n",
    "    \n",
    "    The model is designed to work with input tensors of shape \n",
    "    (batch_size, 32, 32, 32, 1), but it can technically accept any tensor shape.\n",
    "    \n",
    "    Parameters:\n",
    "    - None\n",
    "    \n",
    "    Methods:\n",
    "    - forward(x): Passes the input tensor `x` through the model unchanged.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(IdentityModelPlusNoise, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "        \n",
    "        Parameters:\n",
    "        - x (torch.Tensor): The input tensor with shape (batch_size, 32, 32, 32, 1)\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: The unchanged input tensor.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x)\n",
    "        return x + noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = IdentityModelPlusNoise()\n",
    "batch_size = 10 \n",
    "# Create a dummy input tensor of the specified shape and type\n",
    "input_tensor = torch.randn((batch_size, 32, 32, 32, 1), dtype=torch.float32)\n",
    "\n",
    "# Pass the input tensor through the model\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# Check if the output is the same as the input\n",
    "print(torch.equal(input_tensor, output_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TuckerDecompositionLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A custom loss function based on the Tucker decomposition of tensors.\n",
    "    \n",
    "    The loss is calculated as the Frobenius norm of the difference between the\n",
    "    core tensors of the Tucker decomposition of the predicted and target tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TuckerDecompositionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        # Perform Tucker decomposition on both tensors\n",
    "        loss = 0.0\n",
    "        for i in tqdm(range(len(prediction))):\n",
    "\n",
    "            core_pred, _ = tucker(prediction[i], rank=[5, 5, 5, 5])  # Adjust ranks as needed\n",
    "            core_target, _ = tucker(target[i], rank=[5, 5, 5, 5])  # Adjust ranks as needed\n",
    "        \n",
    "        # Compute the loss as the Frobenius norm between the core tensors\n",
    "            loss += torch.norm(core_pred - core_target, 2)\n",
    "        loss_m = torch.mean(loss) \n",
    "        return loss_m, core_pred, core_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 32, 32, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Assume model, optimizer, data_loader are predefined\n",
    "loss_function = TuckerDecompositionLoss()\n",
    "\n",
    "# Compute loss\n",
    "loss, core_pred, core_target = loss_function(output_tensor, input_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5, 5, 1]), torch.Size([5, 5, 5, 1]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_pred.shape, core_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(610.4222),)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary packages\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # You can replace this with other model names\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input text\n",
    "input_text = \"Where is China?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the attention mask (1 for input tokens, 0 for padding)\n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to('cuda')  # Ensure the mask is on the same device as the model\n",
    "\n",
    "# Set pad_token_id explicitly if it's not defined\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Generate text with the attention_mask and pad_token_id settings\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    pad_token_id=model.config.pad_token_id\n",
    ")\n",
    "\n",
    "# Decoding the output to human-readable text (optional)\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset (a list of sentences)\n",
    "# EDA-themed dataset\n",
    "eda_dataset = [\n",
    "    \"Electronic Design Automation involves complex software algorithms.\",\n",
    "    \"EDA tools are essential for designing integrated circuits.\",\n",
    "    \"In EDA, simulation verifies the functionality of a chip design.\",\n",
    "    \"Layout optimization is a crucial step in chip design.\",\n",
    "    \"EDA software assists engineers in managing power consumption and heat dissipation.\",\n",
    "    \"Routing and placement are key challenges in EDA.\",\n",
    "    \"Machine learning can enhance the automation process in EDA.\",\n",
    "    \"EDA tools are evolving with advancements in semiconductor technology.\",\n",
    "    \"Design rule checking is vital to ensure manufacturing feasibility.\",\n",
    "    \"EDA is critical for the development of efficient and compact electronic devices.\"\n",
    "    \"Deep likes eating paneer.\",\n",
    "    \"Nirjhor lives in Raleigh\", \n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "# Set the pad token to the EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "inputs = tokenizer(eda_dataset, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in tqdm(range(100)):  # number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(**inputs.to(device))\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a test dataset in the format of prompts and expected completions\n",
    "test_dataset = [\n",
    "    {\"prompt\": \"the beans are\", \"expected\": \"\"},\n",
    "    {\"prompt\": \"key challenges are\", \"expected\": \"\"},\n",
    "    {\"prompt\": \"the only way to\", \"expected\": \"\"},\n",
    "    {\"prompt\": \"nirjhor lives\", \"expected\": \"\"}\n",
    "    \n",
    "    # Add more test examples\n",
    "]\n",
    "\n",
    "# Evaluate the model\n",
    "for test_case in test_dataset:\n",
    "    input_ids = tokenizer.encode(test_case[\"prompt\"], return_tensors='pt');\n",
    "    input_ids = input_ids.to(device);\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(input_ids, max_length=50);\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True);\n",
    "\n",
    "    # Compare generated text with expected text\n",
    "    # Here you can apply BLEU score or simply compare the texts manually\n",
    "    print(f\"Prompt: {test_case['prompt']}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(f\"Expected: {test_case['expected']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable parameters: {trainable_params}\")\n",
    "#gpt2 has roughly 124 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
